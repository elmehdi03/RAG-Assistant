##ğŸ¤– RAG Assistant
A Retrieval-Augmented Generation (RAG) assistant that answers questions in natural language using the content of your own documents (PDFs, reports, technical manualsâ€¦).
This project implements a complete RAG pipeline from scratch, without relying on LangChain or LlamaIndex â€” giving full control over ingestion, embedding, retrieval, and response generation.

##ğŸ¯ Objectives


Create an intelligent assistant able to retrieve and summarize knowledge from local documents.


Demonstrate a modular and transparent RAG pipeline using FAISS and SentenceTransformers.


Build an interactive Streamlit interface to query documents in real time.



##ğŸ’¡ Typical Use Cases


ğŸ¦ Regulatory assistant â€“ Basel IV / banking compliance documentation


âš™ï¸ DevOps / technical assistant â€“ internal configuration or process manuals


ğŸ§‘â€ğŸ’¼ Corporate knowledge base â€“ company procedures or internal memos


ğŸ“ Academic or research helper â€“ paper summarization or literature search



##âš™ï¸ Architecture
The pipeline consists of five main components:


Document Ingestion â†’ ingestion.py


Extracts and cleans text from PDF files using PyPDF2


Splits documents into context-preserving chunks




Embeddings Generation â†’ embeddings.py


Uses SentenceTransformers (all-MiniLM-L6-v2) to convert text chunks into dense vectors


Saves embeddings and metadata locally for fast reuse




Vector Search (Retrieval) â†’ faiss.IndexFlatIP


Performs high-speed similarity search using FAISS


Returns the top-k most relevant document chunks




RAG Pipeline â†’ rag_pipeline.py


Combines retrieved context with the userâ€™s query


Generates a contextual response (with a local fallback or future LLM integration)




Web Interface â†’ app.py


Streamlit-based UI with GPU detection, cache validation, and live querying





##ğŸ› ï¸ Tech Stack
CategoryToolsLanguagePython 3.xVector IndexingFAISSEmbeddingsSentenceTransformers (Hugging Face)ParsingPyPDF2InterfaceStreamlitUtilitiesNumPy, Pickle, Torch (CUDA support)

##ğŸ“‚ Project Structure
rag-assistant/
â”œâ”€â”€ data/                    # Document storage
â”‚   â”œâ”€â”€ *.pdf                # Source PDF files
â”‚   â”œâ”€â”€ faiss_index.bin      # FAISS vector index
â”‚   â””â”€â”€ metadata.pkl         # Embedding metadata
â”‚
â”œâ”€â”€ src/                     # Core source code
â”‚   â”œâ”€â”€ ingestion.py         # PDF parsing & cleaning
â”‚   â”œâ”€â”€ embeddings.py        # Embedding generation & FAISS operations
â”‚   â”œâ”€â”€ retriever.py         # Vector search logic
â”‚   â”œâ”€â”€ rag_pipeline.py      # RAG orchestration
â”‚   â””â”€â”€ app.py               # Streamlit web interface
â”‚
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md


ğŸš€ Getting Started
1ï¸âƒ£ Prerequisites
python --version   # Python 3.8+
pip install -r requirements.txt

If FAISS fails to install:
# CPU
pip install faiss-cpu
# or GPU (for RTX 4070 and similar)
pip install faiss-gpu

2ï¸âƒ£ Build the FAISS Index
Place your PDFs inside the data/ folder, then run:
python src/ingestion.py

This will:


Load all documents


Extract and chunk text


Generate embeddings


Build and save the FAISS index (faiss_index.bin)


3ï¸âƒ£ Launch the Web Interface
streamlit run src/app.py

The app will open in your browser at http://localhost:8501
Type a question such as:

â€œWho won the World Cup?â€ or â€œWhat does Basel IV say about credit risk?â€


##âš™ï¸ Configuration
Embedding Model
Default model:
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

You can replace it in embeddings.py with any SentenceTransformer model.
LLM Integration (Optional)
In rag_pipeline.py, replace the placeholder fake_llm() function with your own LLM API call (e.g. OpenAI GPT-4, Mistral, Claude, or a local model via Ollama).

##ğŸ“ˆ Roadmap


 PDF ingestion and cleaning


 SentenceTransformer embeddings


 FAISS indexing and retrieval


 Streamlit interface


 Integration with production LLMs (Mistral / GPT-4)


 Improved semantic chunking


 Source citation display in UI


 Document upload from interface


 Conversation memory


 Dockerization & cloud deployment



##ğŸ§  Example Workflow
from ingestion import load_documents_from_folder
from embeddings import build_faiss_index, search
from rag_pipeline import ragpipeline

# 1. Load and embed documents
texts, metadata = load_documents_from_folder("data")
build_faiss_index(texts, metadata)

# 2. Ask a question
query = "What does Basel IV say about credit risk?"
results = search(query, k=3)
answer = ragpipeline(query)

print(answer)


##ğŸ“œ License
This project is released under the MIT License.
