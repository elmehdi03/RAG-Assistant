# ğŸ¤– RAG Assistant

A production-ready Retrieval-Augmented Generation (RAG) assistant that answers natural-language questions using the content of your own documents (PDFs, reports, manualsâ€¦). This project implements a transparent RAG pipeline from ingestion â†’ embeddings â†’ FAISS retrieval â†’ response generation, without requiring LangChain or LlamaIndex.

âœ¨ Highlights
- ï¿½ Production-Ready: Real LLM integration (Mistral AI), GPU acceleration, professional UI  
- ğŸ“¦ Local-first: documents, embeddings, and index are stored locally for privacy and offline use.  
- ğŸ§© Modular: replace ingestion, embedding model, or LLM easily.  
- ğŸ–¥ï¸ Professional UI: Streamlit-based interface with gradient design, status indicators, and source citations.
- âš¡ GPU-Accelerated: Optimized for NVIDIA GPUs (CUDA 12.1) with CPU fallback
- ğŸ‘ï¸ File Monitoring: Automatic PDF detection and re-indexing via watchdog

Status: **Production-Ready** â€” suitable for deployment and real-world use.

---

Table of Contents
- ğŸš€ Features
- âš¡ Quick Start
- ğŸ—‚ï¸ Project Layout
- ğŸ§  How it works (architecture)
- âš™ï¸ Configuration
- ğŸ§ª Examples
- ğŸ› ï¸ Troubleshooting & Tips
- ğŸ›£ï¸ Roadmap
- ğŸ¤ Contributing
- ğŸ“œ License
- âœ‰ï¸ Contact

---

## ğŸš€ Features
- ğŸ“„ PDF ingestion and text extraction (PyPDF2)  
- âœ‚ï¸ Context-preserving chunking with metadata tracking  
- ğŸ§  GPU-accelerated embeddings using SentenceTransformers (all-MiniLM-L6-v2)  
- ğŸ” FAISS vector store with similarity search (CPU index, GPU-accelerated embeddings)  
- ï¿½ Real LLM integration with Mistral AI (mistral-small model)  
- ğŸŒ Professional Streamlit UI with gradient design, adjustable parameters, and source citations
- ğŸ“Š System status monitoring (GPU, cache validation, PDF count)
- ğŸ‘ï¸ Automatic file monitoring and re-indexing (watchdog)
- âš¡ Full GPU support with CUDA 12.1 and CPU fallback

---

## âš¡ Quick Start (5 minutes)

1ï¸âƒ£ Clone and install
```bash
git clone https://github.com/elmehdi03/rag-assistant.git
cd rag-assistant
python -m venv .venv
source .venv/bin/activate    # on Windows use .venv\Scripts\activate
pip install -r requirements.txt
```

2ï¸âƒ£ Add documents
- Put PDF files in the `data/` directory:
  - `data/your_manual.pdf`
  - `data/other_docs.pdf`

3ï¸âƒ£ Build the FAISS index (ingest, embed, index)
```bash
python src/ingestion.py
```
This will:
- ğŸ“¥ Load PDFs from `data/`
- ğŸ§¼ Extract and clean text
- ğŸ§© Split into chunks (configurable)
- âš™ï¸ Create embeddings and store FAISS index + metadata to `data/`

4ï¸âƒ£ Run the Streamlit app
```bash
streamlit run src/app.py
```
Open http://localhost:8501 in your browser and ask a question like:
- â€œWho won the World Cup?â€ âš½  
- â€œWhat does Basel IV say about credit risk?â€ ğŸ“š

---

## ï¿½ Security & API Configuration

### Setting up Mistral API Key (Secure Method)

**Never commit API keys to git!** This project uses environment variables for secure configuration.

1. **Copy the template:**
   ```bash
   cp .env.example .env
   ```

2. **Get your API key:**
   - Visit [Mistral AI Console](https://console.mistral.ai/)
   - Create an account or sign in
   - Generate a new API key

3. **Add to `.env` file:**
   ```bash
   MISTRAL_API_KEY=your-actual-api-key-here
   ```

4. **Verify it's protected:**
   - The `.env` file is automatically git-ignored
   - Never share or commit this file
   - Each team member should have their own `.env` file

---

## ï¿½ğŸ—‚ï¸ Project layout
rag-assistant/
- data/                    # Document storage & generated index
  - *.pdf                  # Source PDF files
  - faiss_index.bin        # FAISS binary index (generated)
  - metadata.pkl           # Chunk metadata (generated)
- src/
  - ingestion.py           # PDF parsing, cleaning, chunking
  - embeddings.py          # Embedding generation, FAISS operations
  - retriever.py           # Retrieval logic
  - rag_pipeline.py        # Combines context + query and calls an LLM
  - app.py                 # Streamlit UI
- requirements.txt
- LICENSE
- README.md

---

## ğŸ§  How it works (high level)
1. ğŸ“¥ Ingestion: PDF â†’ text â†’ cleaned paragraphs â†’ chunks (context-preserving)  
2. ğŸ§  Embeddings: text chunks â†’ vector embeddings (SentenceTransformers)  
3. ğŸ—ƒï¸ Indexing: FAISS index built from vectors, metadata stored separately  
4. ğŸ” Retrieval: nearest-neighbor search (top-k) returns best chunks  
5. ğŸ“ RAG: retrieved chunks + user prompt are fed to an LLM function (replaceable) to produce an answer, optionally with citations

---

## âš™ï¸ Configuration

Embedding model (default)
- `src/embeddings.py` uses:
  model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
- Swap to any SentenceTransformer model by changing the name or path. ğŸ”

FAISS installation
- CPU:
  ```bash
  pip install faiss-cpu
  ```
- GPU:
  ```bash
  pip install faiss-gpu
  ```
If installation fails, see Troubleshooting below. ğŸ§°

LLM integration
- The repo ships with a simple placeholder (`fake_llm`) for demo responses.
- To use a production LLM:
  - Replace `fake_llm` in `src/rag_pipeline.py` with a function that calls OpenAI, Ollama, Mistral, Claude, etc. â˜ï¸
  - Ensure you handle token limits and truncate or summarize retrieved chunks if needed.

Example: minimal OpenAI integration (conceptual)
```python
# in src/rag_pipeline.py
import os
import openai

openai.api_key = os.getenv("OPENAI_API_KEY")

def call_openai(prompt: str, max_tokens=512, temperature=0.0):
    resp = openai.ChatCompletion.create(
        model="gpt-4o-mini",  # choose the model you have access to
        messages=[{"role": "user", "content": prompt}],
        max_tokens=max_tokens,
        temperature=temperature,
    )
    return resp["choices"][0]["message"]["content"].strip()
```
ğŸ” Keep secrets out of the code; use environment variables.

---

## ğŸ§ª Usage examples (Python)
Programmatic search + answer:
```python
from src.ingestion import load_documents_from_folder
from src.embeddings import build_faiss_index, search
from src.rag_pipeline import ragpipeline

# Build index (one-time)
texts, metadata = load_documents_from_folder("data")
build_faiss_index(texts, metadata)

# Ask a question
query = "What does Basel IV say about credit risk?"
results = search(query, k=3)      # returns nearest chunks
answer = ragpipeline(query)       # contextualized answer
print(answer)
```

Streamlit UI
- The UI shows GPU detection, cache validation, and lets you query the loaded index interactively. ğŸ–±ï¸

---

## ğŸ› ï¸ Troubleshooting & tips
- FAISS install errors:
  - Use `faiss-cpu` if you don't have an NVidia GPU: `pip install faiss-cpu` ğŸ§¾
  - On Linux, ensure `gcc` and `python-dev` headers are installed.
- CUDA / GPU:
  - If using `faiss-gpu`, CUDA drivers and toolkit must match your GPU. ğŸ”Œ
- Large PDFs:
  - Consider increasing chunk size or using an initial text-cleaning pass to remove boilerplate. ğŸ§¹
- Embedding reuse:
  - The embedding step saves metadata and vectors. Re-run ingestion only when documents change. ğŸ”
- Reducing index size:
  - Remove stopwords or apply light normalization before embedding (experimental). ğŸ”¬

---

## ğŸ›£ï¸ Roadmap (planned)
- âœ¨ Improved semantic chunking and adaptive chunk size  
- ğŸ“ Source citation display in UI (show chunk origins)  
- â™»ï¸ Conversation memory (context across turns)  
- ğŸ“¤ Document upload from the UI  
- ğŸ³ Dockerfile and containerized deployment  
- ğŸ”— Integration examples for OpenAI, Ollama, Mistral, and local LLMs  
- âœ… CI automation and tests

---

## ğŸ¤ Contributing
- Contributions welcome! Open an issue or a PR.  
- Suggestion flow:
  1. Create an issue describing the change ğŸ“  
  2. Add tests where relevant âœ…  
  3. Keep changes modular (ingestion, embeddings, retriever, UI) ğŸ› ï¸

Code style / linting
- Prefer small, well-tested changes. Use `black` / `flake8` if adding more code.

---

## ğŸ“œ License
MIT â€” see LICENSE file. ğŸ§¾

---

## ğŸ™ Acknowledgements
- SentenceTransformers (UKPLab / Hugging Face) â¤ï¸  
- FAISS (Facebook AI Research) âš¡  
- Streamlit ğŸŒŠ

---

## âœ‰ï¸ Contact
Maintainer: @elmehdi03  
Report issues at: https://github.com/elmehdi03/rag-assistant/issues
